{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR,ReduceLROnPlateau\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TripletMarginLoss\n",
    "import torch.nn.init as init\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 部署模型，修改分类层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (6): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 使用预训练参数\n",
    "from models.SqueezeNet import SqueezeNet\n",
    "\n",
    "model = SqueezeNet(version=\"1_1\")\n",
    "model.load_state_dict(torch.load('squeezenet1_1_weights.pth'))\n",
    "\n",
    "\n",
    "# 冻结参数层\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 修改最后一层\n",
    "\n",
    "print(model.classifier[1])\n",
    "final_conv = nn.Conv2d(512, 128, kernel_size=1)\n",
    "init.normal_(final_conv.weight, mean=0.0, std=0.01)\n",
    "init.constant_(final_conv.bias, 0.0)\n",
    "model.classifier[1] = final_conv\n",
    "\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "for params in model.classifier[1].parameters():\n",
    "    params.requires_grad = True\n",
    "    \n",
    "# 解冻模型的最后1个Fire模块\n",
    "for param in model.features[-1:].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "    \n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "test_img_001_0 = Image.open('./data_cropped/001/001_0.bmp')\n",
    "test_img_001_1 = Image.open('./data_cropped/001/001_1.bmp')\n",
    "test_img_001_2 = Image.open('./data_cropped/001/001_2.bmp')\n",
    "test_img_003_3 = Image.open('./data_cropped/003/003_3.bmp')\n",
    "test_img_007_3 = Image.open('./data_cropped/007/007_3.bmp')\n",
    "test_img_003_1 = Image.open('./data_cropped/003/003_1.bmp')\n",
    "test_img_003_2 = Image.open('./data_cropped/003/003_2.bmp')\n",
    "\n",
    "lables = [\"001_0\", \"001_1\", \"001_2\", \"003_3\", \"007_3\", \"003_1\", \"003_2\"]\n",
    "\n",
    "transforming = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                                 0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "aligned = []\n",
    "aligned.append(transforming(test_img_001_0))\n",
    "aligned.append(transforming(test_img_001_1))\n",
    "aligned.append(transforming(test_img_001_2))\n",
    "aligned.append(transforming(test_img_003_3))\n",
    "aligned.append(transforming(test_img_007_3))\n",
    "aligned.append(transforming(test_img_003_1))\n",
    "aligned.append(transforming(test_img_003_2))\n",
    "\n",
    "model.eval()\n",
    "aligned = torch.stack(aligned).to('cuda')\n",
    "emdeddings = model(aligned).cpu().detach()\n",
    " \n",
    "print(emdeddings[0].shape)\n",
    "dists = [[(e1 - e2).norm().item() for e2 in emdeddings] for e1 in emdeddings]\n",
    "# pd.DataFrame(dists, columns=lables, index=lables)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取并且定制数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['209', '085', '187', '104', '132', '436', '092', '370', '197', '497', '308', '081', '376', '427', '387', '039', '364', '128', '381', '241', '399', '314', '231', '366', '272', '217', '098', '274', '233', '024', '423', '083', '385', '222', '013', '407', '184', '299', '448', '256', '050', '315', '369', '130', '475', '212', '174', '142', '388', '492', '244', '340', '397', '208', '418', '007', '019', '305', '390', '166', '040', '037', '474', '421', '043', '065', '077', '135', '361', '260', '117', '080', '449', '118', '106', '405', '220', '298', '099', '269', '263', '120', '084', '284', '488', '071', '349', '154', '334', '482', '408', '086', '317', '447', '090', '451', '477', '277', '345', '249', '267', '450', '246', '297', '147', '079', '278', '097', '171', '355', '498', '283', '001', '012', '280', '032', '069', '392', '304', '144', '161', '044', '188', '185', '030', '243', '033', '362', '316', '219', '078', '138', '202', '133', '411', '111', '215', '074', '136', '331', '172', '207', '175', '458', '469', '239', '193', '301', '335', '303', '076', '425', '123', '067', '389', '102', '179', '466', '108', '238', '321', '417', '191', '164', '493', '157', '426', '255', '307', '383', '058', '434', '169', '005', '339', '137', '228', '354', '431', '326', '053', '126', '464', '463', '095', '205', '075', '041', '350', '151', '391', '437', '008', '131', '330', '295', '101', '337', '213', '479', '319', '027', '143', '424', '036', '318', '442', '021', '441', '031', '360', '236', '292', '494', '141', '440', '342', '011', '113', '270', '259', '401', '004', '483', '328', '338', '443', '002', '250', '049', '023', '042', '225', '064', '395', '456', '073', '237', '257', '035', '430', '444', '422', '416', '462', '100', '336', '139', '034', '046', '465', '403', '375', '275', '216', '495', '486', '258', '227', '396', '116', '226', '153', '234', '159', '276', '439', '180', '223', '038', '268', '210', '320', '066', '471', '052', '145', '163', '242', '070', '051', '379', '468', '028', '467', '281', '181', '167', '311', '429', '195', '022', '325', '107', '485', '176', '127', '415', '094', '055', '368', '352', '296', '414', '091', '402', '245', '265', '124', '152', '253', '221', '186', '476', '230', '119', '333', '374', '178', '322', '484', '026', '229', '177', '353', '459', '400', '365', '372', '261', '105', '082', '499', '460', '313', '457', '129', '329', '410', '406', '199', '445', '367', '006', '309', '394', '029', '252', '487', '003', '412', '089', '264', '125', '203', '279', '025', '114', '122', '182', '478', '201', '351', '455', '140', '115', '419', '146', '306', '251', '271', '294', '373', '491', '010', '344', '087', '363', '045', '148', '432', '240', '287', '015', '472', '386', '072', '189', '020', '158', '192', '377', '018', '059', '300', '357', '490', '134', '109', '063', '194', '214', '413', '093', '218', '480', '224', '454', '014', '343', '378', '190', '358', '324', '288', '156', '409', '496', '420', '057', '470', '356', '009', '384', '289', '312', '056', '435', '211', '017', '096', '149', '433', '393', '446', '160', '254', '382', '206', '310', '332', '054', '489', '247', '150', '481', '262', '453', '380', '235', '323', '110', '341', '273', '286', '060', '204', '461', '285', '232', '438', '359', '248', '170', '061', '198', '162', '327', '200', '346', '168', '112', '282', '173', '347', '048', '062', '183', '348', '428', '016', '290', '404', '165', '293', '266', '371', '473', '398', '047', '196', '452', '088', '121', '103', '302', '068', '291', '155']\n",
      "['391', '417', '258', '245', '187', '287', '254', '499', '045', '091', '264', '024', '162', '108', '189', '124', '389', '355', '057', '119', '432', '270', '478', '233', '072', '076', '117', '197', '439', '283', '278', '058', '247', '385', '407', '203', '087', '430', '393', '103', '211', '246', '392', '279', '380', '129', '030', '200', '085', '013', '136', '336', '032', '409', '204', '293', '133', '082', '112', '014', '445', '181', '069', '046', '404', '231', '041', '424', '054', '120', '193', '130', '001', '113', '153', '335', '025', '491', '370', '176', '481', '107', '140', '122', '367', '297', '347', '114', '493', '100', '115', '060', '312', '066', '267', '397', '400', '228', '464', '263', '311', '304', '188', '026', '476', '360', '431', '172', '132', '496', '237', '022', '309', '356', '006', '440', '165', '065', '441', '480', '319', '156', '368', '420', '068', '456', '061', '217', '190', '341', '049', '475', '396', '016', '195', '352', '128', '238', '110', '433', '457', '329', '461', '354', '135', '272', '079', '059', '208', '471', '078', '131', '255', '196', '093', '157', '044', '301', '214', '230', '435', '412', '372', '094', '194', '273', '383', '051', '384', '177', '477', '090', '109', '099', '186', '067', '280', '337', '274', '463', '123', '137', '236', '452', '234', '167', '434', '261', '052', '232', '422', '366', '036', '488', '406', '221', '241', '284', '023', '350', '455', '175', '450', '398', '289', '212', '055', '345', '142', '158', '423', '213', '171', '243', '485', '018', '215', '333', '436', '252', '043', '444', '166', '388', '414', '027', '225', '179', '139', '381', '199', '421', '081', '216', '343', '437', '324', '073', '490', '269', '202', '305', '086', '160', '173', '427', '084', '105', '302', '227', '031', '351', '005', '390', '147', '469', '340', '210', '315', '418', '458', '256', '353', '116', '494', '497', '282', '484', '251', '374', '318', '062', '048', '323', '008', '071', '466', '242', '303', '320', '470', '285', '395', '089', '248', '316', '145', '408', '379', '224', '239', '144', '164', '331', '460', '178', '447', '349', '262', '459', '039', '346', '328', '344', '308', '495', '413', '472', '010', '012', '155', '487', '357', '220', '290', '206', '040', '268', '365', '185', '373', '138', '209', '092', '097', '419', '286', '009', '148', '118', '277', '275', '483', '387', '223', '015', '126', '159', '454', '003', '492', '035', '240', '028', '382', '486', '429', '250', '467', '201', '021', '306', '310', '134', '325', '180', '330', '019', '080', '102', '146', '473', '047', '101', '307', '125', '161', '075', '266', '425', '298', '299', '359', '104', '416', '184', '405', '106', '294', '364', '152', '334', '361', '002', '183', '394', '007', '011', '362', '074', '442', '415', '253', '168', '064', '042', '426', '056', '229']\n",
      "-------\n",
      "['053', '182', '371', '154', '428', '218', '326', '170', '281', '295', '088', '149', '348', '375', '321', '235', '063', '411', '020', '339', '322', '314', '276', '438', '446', '403', '401', '111', '474', '151', '449', '096', '249', '317', '037', '127', '300', '143', '465', '378', '448', '498', '376', '342', '462', '077', '174', '121', '296', '410', '292', '402', '029', '338', '259', '468', '358', '291', '222', '288', '260', '141', '377', '207', '313', '386', '482', '443', '226', '332', '191', '244', '095', '205', '399', '271', '369', '198', '192', '489', '169', '453', '017', '363', '034', '150', '265', '070', '479', '219', '083', '163', '257', '038', '050', '327', '033', '004', '098', '451']\n"
     ]
    }
   ],
   "source": [
    "loss = TripletMarginLoss(margin=0.2, p=2)\n",
    "\n",
    "\n",
    "# 分割三元组数据集\n",
    "class TripletFaceDataset(Dataset):\n",
    "    def __init__(self, image_folder,persons,transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.persons = persons\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        anchor_person = random.choice(self.persons)\n",
    "        positive_person = anchor_person\n",
    "        negative_person = random.choice(self.persons)\n",
    "        while negative_person == anchor_person:\n",
    "            negative_person = random.choice(self.persons)\n",
    "\n",
    "        anchor_img_path = random.choice(os.listdir(\n",
    "            os.path.join(self.image_folder, anchor_person)))\n",
    "        positive_img_path = random.choice(os.listdir(\n",
    "            os.path.join(self.image_folder, positive_person)))\n",
    "        negative_img_path = random.choice(os.listdir(\n",
    "            os.path.join(self.image_folder, negative_person)))\n",
    "\n",
    "        anchor_img = Image.open(os.path.join(\n",
    "            self.image_folder, anchor_person, anchor_img_path))\n",
    "        positive_img = Image.open(os.path.join(\n",
    "            self.image_folder, anchor_person, positive_img_path))\n",
    "        negative_img = Image.open(os.path.join(\n",
    "            self.image_folder, negative_person, negative_img_path))\n",
    "\n",
    "        # 进行数据处理\n",
    "        transform = self.transform\n",
    "\n",
    "        anchor_img = transform(anchor_img)\n",
    "        positive_img = transform(positive_img)\n",
    "        negative_img = transform(negative_img)\n",
    "\n",
    "        return anchor_img, positive_img, negative_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.persons) * 5  # 假设每个人有5张图片\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "# 切割数据集八二分\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "workers = 0 if os.name == 'nt' else 8\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder('./data_cropped', transform=transforming)\n",
    "labels = os.listdir('./data_cropped')\n",
    "print(labels)\n",
    "random.shuffle(labels)\n",
    "train_idx, test_idx = train_test_split(\n",
    "    labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_idx)\n",
    "print('-------')\n",
    "print(test_idx)\n",
    "\n",
    "train_dataset = TripletFaceDataset(\n",
    "    image_folder='./data_cropped', persons=train_idx, transform=transforming\n",
    "    )\n",
    "\n",
    "test_dataset = TripletFaceDataset(\n",
    "    image_folder='./data_cropped', persons=test_idx, transform=transforming\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhy/anaconda3/envs/squeezenet/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# 定制优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# def lambda_rule(epoch,max_epoch):\n",
    "#     max_epoch = 20\n",
    "#     return (1-epoch/max_epoch)**0.9 ##多项式衰减\n",
    "\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "# 定制参考嵌入向量\n",
    "reference_embeddings = []\n",
    "reference_labels = []\n",
    "model.eval()\n",
    "# 遍历数据集每个文件夹，每个文件夹embeddings计算平均值，放进reference_embeddings,对应的标签放进reference_labels\n",
    "for label in labels:\n",
    "    label_embeddings = []\n",
    "    for img_path in os.listdir(os.path.join('./data_cropped', label)):\n",
    "        img = Image.open(os.path.join('./data_cropped', label, img_path))\n",
    "        img = transforming(img).to('cuda')\n",
    "        img = img.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(img).cpu().detach().numpy()\n",
    "            label_embeddings.append(embedding)    \n",
    "    label_embeddings = np.array(label_embeddings)\n",
    "    np_mean = np.mean(label_embeddings, axis=0)\n",
    "    tensor_mean = torch.from_numpy(np_mean).to('cuda')\n",
    "    reference_embeddings.append(tensor_mean)\n",
    "    reference_labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "----------\n",
      "Accuracy on test set: 0.0060\n",
      "Accuracy on test set: 0.0140\n",
      "Accuracy on test set: 0.0140\n",
      "Accuracy on test set: 0.0100\n",
      "Accuracy on test set: 0.0140\n",
      "Accuracy on test set: 0.0060\n",
      "Accuracy on test set: 0.0060\n",
      "Accuracy on test set: 0.0040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m minidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, reference_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reference_embeddings):\n\u001b[0;32m---> 55\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpairwise_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;241m<\u001b[39m mindist:\n\u001b[1;32m     57\u001b[0m         mindist \u001b[38;5;241m=\u001b[39m dist\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/triplet_loss_experiment')\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('Start Training')\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\n循环 {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        anchor, positive, negative = data\n",
    "        anchor, positive, negative = anchor.to('cuda'), positive.to('cuda'), negative.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        anchor_out = model(anchor)\n",
    "        positive_out = model(positive)\n",
    "        negative_out = model(negative)\n",
    "        loss_val = loss(anchor_out, positive_out, negative_out)\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss_val.item()\n",
    "        writer.iteration += 1\n",
    "        print(f\"the loss is {running_loss}\")\n",
    "        if i % 10 == 9:\n",
    "            writer.add_scalar('loss', running_loss / 10,\n",
    "                              writer.iteration)\n",
    "            running_loss = 0.0\n",
    "    print('Epoch{} finished'.format(epoch))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    ## we use acc to evaluate the model\n",
    "    with torch.no_grad():\n",
    "        # 用标签来评估模型\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        # use labels to evaluate the model\n",
    "        for i,data in enumerate(test_loader,0):\n",
    "                anchor, _, _ = data\n",
    "                batch_sizes = anchor.size(0)\n",
    "                total += batch_sizes\n",
    "                anchor = anchor.to('cuda')\n",
    "                # 找到anchor 的标签\n",
    "                anchor_label = os.path.basename(test_loader.dataset.persons[i])\n",
    "                # print(anchor.shape)\n",
    "                anchor_out = model(anchor) \n",
    "                         \n",
    "                for j in range(batch_sizes):\n",
    "                    anchor_embedding = anchor_out[j].unsqueeze(0)\n",
    "                    mindist = torch.full((1,), float('inf')).to('cuda')\n",
    "                    minidx = -1\n",
    "                    for i, reference_embedding in enumerate(reference_embeddings):\n",
    "                        dist = F.pairwise_distance(anchor_embedding, reference_embedding, p=2)\n",
    "                        if dist < mindist:\n",
    "                            mindist = dist\n",
    "                            minidx = i\n",
    "                                    \n",
    "                                    \n",
    "                    predict_label = reference_labels[minidx]\n",
    "                    if predict_label == anchor_label:\n",
    "                        correct += 1\n",
    "                    \n",
    "                \n",
    "                    \n",
    "        acc = correct / total\n",
    "        print('Accuracy on test set: {:.4f}'.format(acc))\n",
    "        writer.add_scalar('acc', acc, epoch)\n",
    "        \n",
    "        \n",
    "        # 用scheduler来更新学习率\n",
    "        scheduler.step(acc)\n",
    "        print('lr:', optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "print('Finished Training')\n",
    "\n",
    "writer.close()\n",
    "\n",
    "\n",
    "                \n",
    "    \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "              \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "squeezenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
