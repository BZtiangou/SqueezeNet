{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import LambdaLR,ReduceLROnPlateau\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TripletMarginLoss\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch.nn.init as init\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 部署模型，修改分类层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (6): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 使用预训练参数\n",
    "from models.SqueezeNet import SqueezeNet\n",
    "\n",
    "model = SqueezeNet(version=\"1_1\")\n",
    "model.load_state_dict(torch.load('squeezenet1_1_weights.pth'))\n",
    "\n",
    "\n",
    "# 冻结参数层\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 修改最后一层\n",
    "\n",
    "print(model.classifier[1])\n",
    "final_conv = nn.Conv2d(512, 128, kernel_size=1)\n",
    "init.normal_(final_conv.weight, mean=0.0, std=0.01)\n",
    "init.constant_(final_conv.bias, 0.0)\n",
    "model.classifier[1] = final_conv\n",
    "\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "for params in model.classifier[1].parameters():\n",
    "    params.requires_grad = True\n",
    "    \n",
    "# 解冻模型的最后1个Fire模块\n",
    "for param in model.features[-1:].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "    \n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "test_img_001_0 = Image.open('./data_cropped/001/001_0.bmp')\n",
    "test_img_001_1 = Image.open('./data_cropped/001/001_1.bmp')\n",
    "test_img_001_2 = Image.open('./data_cropped/001/001_2.bmp')\n",
    "test_img_003_3 = Image.open('./data_cropped/003/003_3.bmp')\n",
    "test_img_007_3 = Image.open('./data_cropped/007/007_3.bmp')\n",
    "test_img_003_1 = Image.open('./data_cropped/003/003_1.bmp')\n",
    "test_img_003_2 = Image.open('./data_cropped/003/003_2.bmp')\n",
    "\n",
    "lables = [\"001_0\", \"001_1\", \"001_2\", \"003_3\", \"007_3\", \"003_1\", \"003_2\"]\n",
    "\n",
    "transforming = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                                 0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "aligned = []\n",
    "aligned.append(transforming(test_img_001_0))\n",
    "aligned.append(transforming(test_img_001_1))\n",
    "aligned.append(transforming(test_img_001_2))\n",
    "aligned.append(transforming(test_img_003_3))\n",
    "aligned.append(transforming(test_img_007_3))\n",
    "aligned.append(transforming(test_img_003_1))\n",
    "aligned.append(transforming(test_img_003_2))\n",
    "\n",
    "model.eval()\n",
    "aligned = torch.stack(aligned).to('cuda')\n",
    "emdeddings = model(aligned).cpu().detach()\n",
    " \n",
    "print(emdeddings[0].shape)\n",
    "dists = [[(e1 - e2).norm().item() for e2 in emdeddings] for e1 in emdeddings]\n",
    "# pd.DataFrame(dists, columns=lables, index=lables)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取并且定制数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['209', '085', '187', '104', '132', '436', '092', '370', '197', '497', '308', '081', '376', '427', '387', '039', '364', '128', '381', '241', '399', '314', '231', '366', '272', '217', '098', '274', '233', '024', '423', '083', '385', '222', '013', '407', '184', '299', '448', '256', '050', '315', '369', '130', '475', '212', '174', '142', '388', '492', '244', '340', '397', '208', '418', '007', '019', '305', '390', '166', '040', '037', '474', '421', '043', '065', '077', '135', '361', '260', '117', '080', '449', '118', '106', '405', '220', '298', '099', '269', '263', '120', '084', '284', '488', '071', '349', '154', '334', '482', '408', '086', '317', '447', '090', '451', '477', '277', '345', '249', '267', '450', '246', '297', '147', '079', '278', '097', '171', '355', '498', '283', '001', '012', '280', '032', '069', '392', '304', '144', '161', '044', '188', '185', '030', '243', '033', '362', '316', '219', '078', '138', '202', '133', '411', '111', '215', '074', '136', '331', '172', '207', '175', '458', '469', '239', '193', '301', '335', '303', '076', '425', '123', '067', '389', '102', '179', '466', '108', '238', '321', '417', '191', '164', '493', '157', '426', '255', '307', '383', '058', '434', '169', '005', '339', '137', '228', '354', '431', '326', '053', '126', '464', '463', '095', '205', '075', '041', '350', '151', '391', '437', '008', '131', '330', '295', '101', '337', '213', '479', '319', '027', '143', '424', '036', '318', '442', '021', '441', '031', '360', '236', '292', '494', '141', '440', '342', '011', '113', '270', '259', '401', '004', '483', '328', '338', '443', '002', '250', '049', '023', '042', '225', '064', '395', '456', '073', '237', '257', '035', '430', '444', '422', '416', '462', '100', '336', '139', '034', '046', '465', '403', '375', '275', '216', '495', '486', '258', '227', '396', '116', '226', '153', '234', '159', '276', '439', '180', '223', '038', '268', '210', '320', '066', '471', '052', '145', '163', '242', '070', '051', '379', '468', '028', '467', '281', '181', '167', '311', '429', '195', '022', '325', '107', '485', '176', '127', '415', '094', '055', '368', '352', '296', '414', '091', '402', '245', '265', '124', '152', '253', '221', '186', '476', '230', '119', '333', '374', '178', '322', '484', '026', '229', '177', '353', '459', '400', '365', '372', '261', '105', '082', '499', '460', '313', '457', '129', '329', '410', '406', '199', '445', '367', '006', '309', '394', '029', '252', '487', '003', '412', '089', '264', '125', '203', '279', '025', '114', '122', '182', '478', '201', '351', '455', '140', '115', '419', '146', '306', '251', '271', '294', '373', '491', '010', '344', '087', '363', '045', '148', '432', '240', '287', '015', '472', '386', '072', '189', '020', '158', '192', '377', '018', '059', '300', '357', '490', '134', '109', '063', '194', '214', '413', '093', '218', '480', '224', '454', '014', '343', '378', '190', '358', '324', '288', '156', '409', '496', '420', '057', '470', '356', '009', '384', '289', '312', '056', '435', '211', '017', '096', '149', '433', '393', '446', '160', '254', '382', '206', '310', '332', '054', '489', '247', '150', '481', '262', '453', '380', '235', '323', '110', '341', '273', '286', '060', '204', '461', '285', '232', '438', '359', '248', '170', '061', '198', '162', '327', '200', '346', '168', '112', '282', '173', '347', '048', '062', '183', '348', '428', '016', '290', '404', '165', '293', '266', '371', '473', '398', '047', '196', '452', '088', '121', '103', '302', '068', '291', '155']\n",
      "['413', '028', '279', '372', '305', '112', '467', '425', '190', '087', '489', '345', '410', '369', '226', '323', '433', '128', '175', '277', '162', '178', '043', '405', '361', '302', '280', '354', '373', '351', '212', '341', '154', '107', '241', '272', '491', '478', '045', '121', '461', '071', '452', '307', '124', '379', '086', '469', '365', '165', '068', '100', '383', '301', '225', '447', '468', '300', '263', '253', '474', '380', '075', '328', '235', '376', '374', '308', '378', '434', '486', '018', '403', '429', '298', '438', '014', '402', '428', '171', '464', '096', '266', '317', '285', '414', '400', '005', '498', '003', '418', '182', '229', '126', '161', '330', '492', '148', '034', '181', '404', '325', '256', '059', '333', '342', '473', '340', '069', '222', '255', '089', '078', '072', '234', '382', '219', '370', '193', '016', '292', '177', '007', '388', '013', '247', '106', '479', '134', '044', '119', '115', '275', '463', '180', '205', '076', '385', '213', '236', '475', '453', '048', '168', '210', '259', '480', '164', '254', '338', '443', '449', '484', '065', '377', '248', '047', '050', '127', '497', '472', '011', '046', '214', '160', '083', '001', '304', '070', '426', '267', '129', '149', '125', '332', '288', '117', '321', '023', '460', '159', '435', '319', '381', '136', '243', '173', '420', '141', '155', '080', '289', '392', '073', '094', '146', '163', '188', '191', '270', '293', '130', '184', '359', '252', '242', '310', '017', '331', '240', '102', '393', '444', '170', '153', '084', '133', '067', '095', '439', '362', '485', '233', '348', '093', '114', '172', '174', '337', '334', '097', '309', '386', '029', '157', '074', '239', '166', '320', '423', '375', '417', '103', '132', '396', '445', '085', '287', '324', '290', '261', '470', '135', '032', '250', '108', '278', '251', '339', '055', '041', '060', '258', '459', '200', '020', '081', '481', '466', '257', '482', '158', '138', '063', '156', '204', '483', '412', '187', '399', '271', '397', '442', '052', '336', '451', '211', '291', '027', '411', '437', '406', '142', '040', '039', '303', '053', '064', '284', '220', '150', '430', '387', '398', '493', '499', '347', '281', '476', '057', '198', '450', '408', '419', '061', '357', '455', '311', '465', '144', '299', '322', '477', '197', '495', '391', '022', '179', '471', '306', '313', '364', '186', '286', '356', '104', '401', '196', '025', '202', '366', '249', '421', '487', '189', '049', '090', '367', '274', '140', '238', '395', '268', '422', '245', '221', '120', '139', '244', '360', '436', '038', '352', '456', '066', '318', '169', '218', '209', '037', '111', '030', '368', '273', '358', '203', '237', '389', '490', '343', '123', '082', '407', '207', '350', '010', '015', '295', '458', '101', '012', '092', '195', '246', '454', '440', '231', '297', '145']\n",
      "-------\n",
      "['415', '024', '276', '035', '424', '216', '099', '143', '131', '315', '371', '283', '098', '269', '394', '223', '363', '194', '232', '167', '079', '199', '201', '355', '390', '176', '110', '137', '006', '448', '208', '496', '021', '151', '432', '462', '409', '077', '316', '118', '327', '446', '031', '056', '488', '494', '051', '384', '026', '019', '062', '353', '116', '054', '441', '152', '344', '294', '105', '312', '206', '260', '329', '326', '230', '033', '346', '227', '036', '004', '427', '264', '335', '122', '147', '228', '262', '282', '217', '192', '002', '183', '113', '009', '088', '314', '349', '416', '431', '091', '185', '215', '265', '224', '109', '457', '042', '296', '058', '008']\n"
     ]
    }
   ],
   "source": [
    "loss = TripletMarginLoss(margin=0.3, p=2)\n",
    "\n",
    "\n",
    "# 分割三元组数据集\n",
    "class TripletFaceDataset(Dataset):\n",
    "    def __init__(self, image_folder,persons,transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.persons = persons\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        anchor_person = random.choice(self.persons)\n",
    "        positive_person = anchor_person\n",
    "        negative_person = random.choice(self.persons)\n",
    "        while negative_person == anchor_person:\n",
    "            negative_person = random.choice(self.persons)\n",
    "\n",
    "        anchor_img_path = random.choice(os.listdir(\n",
    "            os.path.join(self.image_folder, anchor_person)))\n",
    "        positive_img_path = random.choice(os.listdir(\n",
    "            os.path.join(self.image_folder, positive_person)))\n",
    "        negative_img_path = random.choice(os.listdir(\n",
    "            os.path.join(self.image_folder, negative_person)))\n",
    "\n",
    "        anchor_img = Image.open(os.path.join(\n",
    "            self.image_folder, anchor_person, anchor_img_path))\n",
    "        positive_img = Image.open(os.path.join(\n",
    "            self.image_folder, anchor_person, positive_img_path))\n",
    "        negative_img = Image.open(os.path.join(\n",
    "            self.image_folder, negative_person, negative_img_path))\n",
    "\n",
    "        # 进行数据处理\n",
    "        transform = self.transform\n",
    "\n",
    "        anchor_img = transform(anchor_img)\n",
    "        positive_img = transform(positive_img)\n",
    "        negative_img = transform(negative_img)\n",
    "\n",
    "        return (anchor_img, positive_img, negative_img),(anchor_person, positive_person, negative_person)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.persons) * 5  # 假设每个人有5张图片\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "# 切割数据集八二分\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "workers = 0 if os.name == 'nt' else 8\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder('./data_cropped', transform=transforming)\n",
    "labels = os.listdir('./data_cropped')\n",
    "print(labels)\n",
    "random.shuffle(labels)\n",
    "train_idx, test_idx = train_test_split(\n",
    "    labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = TripletFaceDataset(\n",
    "    image_folder='./data_cropped', persons=train_idx, transform=transforming\n",
    "    )\n",
    "\n",
    "test_dataset = TripletFaceDataset(\n",
    "    image_folder='./data_cropped', persons=test_idx, transform=transforming\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhy/anaconda3/envs/squeezenet/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# 定制优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# def lambda_rule(epoch,max_epoch):\n",
    "#     max_epoch = 20\n",
    "#     return (1-epoch/max_epoch)**0.9 ##多项式衰减\n",
    "\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10)\n",
    "\n",
    "\n",
    "# 定制参考嵌入向量\n",
    "reference_embeddings = []\n",
    "reference_labels = []\n",
    "model.eval()\n",
    "# 遍历数据集每个文件夹，每个文件夹embeddings计算平均值，放进reference_embeddings,对应的标签放进reference_labels\n",
    "for label in labels:\n",
    "    label_embeddings = []\n",
    "    for img_path in os.listdir(os.path.join('./data_cropped', label)):\n",
    "        img = Image.open(os.path.join('./data_cropped', label, img_path))\n",
    "        img = transforming(img).to('cuda')\n",
    "        img = img.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(img).cpu().detach().numpy()\n",
    "            label_embeddings.append(embedding)    \n",
    "    label_embeddings = np.array(label_embeddings)\n",
    "    np_mean = np.mean(label_embeddings, axis=0)\n",
    "    tensor_mean = torch.from_numpy(np_mean).to('cuda')\n",
    "    reference_embeddings.append(tensor_mean)\n",
    "    reference_labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "----------\n",
      "\n",
      "循环 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss is 0.13387802243232727\n",
      "the loss is 0.2712397277355194\n",
      "the loss is 0.45408427715301514\n",
      "the loss is 0.6154322624206543\n",
      "the loss is 0.7360809743404388\n",
      "the loss is 0.8768720328807831\n",
      "the loss is 1.0507438480854034\n",
      "the loss is 1.1260280534625053\n",
      "the loss is 1.21124629676342\n",
      "the loss is 1.2634118273854256\n",
      "the loss is 0.22343415021896362\n",
      "the loss is 0.3251304402947426\n",
      "the loss is 0.38285718113183975\n",
      "the loss is 0.537703312933445\n",
      "the loss is 0.5800512954592705\n",
      "the loss is 0.685709722340107\n",
      "the loss is 0.6984297782182693\n",
      "the loss is 1.0367788225412369\n",
      "the loss is 1.083562508225441\n",
      "the loss is 1.2232123464345932\n",
      "the loss is 0.11082091182470322\n",
      "the loss is 0.28617649525403976\n",
      "the loss is 0.2871822193264961\n",
      "the loss is 0.3419572338461876\n",
      "the loss is 0.42946167290210724\n",
      "the loss is 0.5750168710947037\n",
      "the loss is 0.6703439205884933\n",
      "the loss is 0.7709489017724991\n",
      "the loss is 0.8533849790692329\n",
      "the loss is 0.9389441609382629\n",
      "the loss is 0.08719602972269058\n",
      "the loss is 0.16390536725521088\n",
      "the loss is 0.29063475131988525\n",
      "the loss is 0.3264530077576637\n",
      "the loss is 0.4109843373298645\n",
      "the loss is 0.5725852847099304\n",
      "the loss is 0.6444278135895729\n",
      "the loss is 0.7599077820777893\n",
      "the loss is 0.8327976316213608\n",
      "the loss is 0.9118940755724907\n",
      "the loss is 0.03638254106044769\n",
      "the loss is 0.0941384807229042\n",
      "the loss is 0.19097484648227692\n",
      "the loss is 0.2663877382874489\n",
      "the loss is 0.3248719051480293\n",
      "the loss is 0.4086867645382881\n",
      "the loss is 0.47340119630098343\n",
      "the loss is 0.5004780143499374\n",
      "the loss is 0.52793850004673\n",
      "the loss is 0.5806897431612015\n",
      "the loss is 0.004064567387104034\n",
      "the loss is 0.1070038229227066\n",
      "the loss is 0.16040177643299103\n",
      "the loss is 0.2772230878472328\n",
      "the loss is 0.3005722165107727\n",
      "the loss is 0.37826015800237656\n",
      "the loss is 0.4130931794643402\n",
      "the loss is 0.4425766319036484\n",
      "the loss is 0.5082032680511475\n",
      "the loss is 0.5884497687220573\n",
      "the loss is 0.04874248802661896\n",
      "the loss is 0.09027451276779175\n",
      "the loss is 0.16943975538015366\n",
      "Epoch0 finished\n",
      "acc: 0.0\n",
      "correct: 0\n",
      "lr: 0.001\n",
      "\n",
      "循环 2/20\n",
      "----------\n",
      "the loss is 0.0730673149228096\n",
      "the loss is 0.14146354794502258\n",
      "the loss is 0.23133431375026703\n",
      "the loss is 0.2595350742340088\n",
      "the loss is 0.3664218410849571\n",
      "the loss is 0.41472314298152924\n",
      "the loss is 0.445609413087368\n",
      "the loss is 0.595044307410717\n",
      "the loss is 0.6614046767354012\n",
      "the loss is 0.7701445966959\n",
      "the loss is 0.05521444231271744\n",
      "the loss is 0.1491158828139305\n",
      "the loss is 0.154481403529644\n",
      "the loss is 0.18462206423282623\n",
      "the loss is 0.21691828221082687\n",
      "the loss is 0.26555629819631577\n",
      "the loss is 0.31427524983882904\n",
      "the loss is 0.31541023403406143\n",
      "the loss is 0.39115577936172485\n",
      "the loss is 0.45847102999687195\n",
      "the loss is 0.013507038354873657\n",
      "the loss is 0.05174720287322998\n",
      "the loss is 0.08970064669847488\n",
      "the loss is 0.12731683999300003\n",
      "the loss is 0.32607025653123856\n",
      "the loss is 0.3624712824821472\n",
      "the loss is 0.36572951078414917\n",
      "the loss is 0.44654928892850876\n",
      "the loss is 0.5852606818079948\n",
      "the loss is 0.7036738321185112\n",
      "the loss is 0.0200892835855484\n",
      "the loss is 0.07014104723930359\n",
      "the loss is 0.08816660940647125\n",
      "the loss is 0.18787601590156555\n",
      "the loss is 0.2400396540760994\n",
      "the loss is 0.26596932113170624\n",
      "the loss is 0.2911558151245117\n",
      "the loss is 0.32008879631757736\n",
      "the loss is 0.3494667261838913\n",
      "the loss is 0.37964796274900436\n",
      "the loss is 0.0362456813454628\n",
      "the loss is 0.07932768017053604\n",
      "the loss is 0.12130747735500336\n",
      "the loss is 0.15598707646131516\n",
      "the loss is 0.261274553835392\n",
      "the loss is 0.30522432923316956\n",
      "the loss is 0.3088093250989914\n",
      "the loss is 0.3600345030426979\n",
      "the loss is 0.3999854400753975\n",
      "the loss is 0.45392250269651413\n",
      "the loss is 0.03745025396347046\n",
      "the loss is 0.040685154497623444\n",
      "the loss is 0.058600232005119324\n",
      "the loss is 0.0798024833202362\n",
      "the loss is 0.1220589280128479\n",
      "the loss is 0.2018801048398018\n",
      "the loss is 0.23539778590202332\n",
      "the loss is 0.23539778590202332\n",
      "the loss is 0.25930970162153244\n",
      "the loss is 0.44481316953897476\n",
      "the loss is 0.013959743082523346\n",
      "the loss is 0.039693914353847504\n",
      "the loss is 0.0954410582780838\n",
      "Epoch1 finished\n",
      "acc: 0.006\n",
      "correct: 3\n",
      "lr: 0.001\n",
      "\n",
      "循环 3/20\n",
      "----------\n",
      "the loss is 0.01935277134180069\n",
      "the loss is 0.07748229801654816\n",
      "the loss is 0.07748229801654816\n",
      "the loss is 0.17461711913347244\n",
      "the loss is 0.2177623212337494\n",
      "the loss is 0.22866754978895187\n",
      "the loss is 0.2798093408346176\n",
      "the loss is 0.37061329931020737\n",
      "the loss is 0.5501548573374748\n",
      "the loss is 0.6624953895807266\n",
      "the loss is 0.04888799786567688\n",
      "the loss is 0.06408809870481491\n",
      "the loss is 0.09776081889867783\n",
      "the loss is 0.17981936037540436\n",
      "the loss is 0.21236327290534973\n",
      "the loss is 0.2600152716040611\n",
      "the loss is 0.26152752339839935\n",
      "the loss is 0.27142928540706635\n",
      "the loss is 0.36781661212444305\n",
      "the loss is 0.3970026671886444\n",
      "the loss is 0.05118170380592346\n",
      "the loss is 0.10395054519176483\n",
      "the loss is 0.1161731630563736\n",
      "the loss is 0.1161731630563736\n",
      "the loss is 0.1446215659379959\n",
      "the loss is 0.1477384716272354\n",
      "the loss is 0.1776530221104622\n",
      "the loss is 0.18558508902788162\n",
      "the loss is 0.2491992935538292\n",
      "the loss is 0.2813255786895752\n",
      "the loss is 0.023026101291179657\n",
      "the loss is 0.07450585812330246\n",
      "the loss is 0.07450585812330246\n",
      "the loss is 0.13499369472265244\n",
      "the loss is 0.21916913986206055\n",
      "the loss is 0.22915390133857727\n",
      "the loss is 0.2880691587924957\n",
      "the loss is 0.41763168573379517\n",
      "the loss is 0.5044036880135536\n",
      "the loss is 0.5290065780282021\n",
      "the loss is 0.007643267512321472\n",
      "the loss is 0.03132975846529007\n",
      "the loss is 0.13468119502067566\n",
      "the loss is 0.16457179188728333\n",
      "the loss is 0.31845158338546753\n",
      "the loss is 0.31932638585567474\n",
      "the loss is 0.3322358652949333\n",
      "the loss is 0.3443249315023422\n",
      "the loss is 0.34634409844875336\n",
      "the loss is 0.39965489506721497\n",
      "the loss is 0.0\n",
      "the loss is 0.014556676149368286\n",
      "the loss is 0.030246831476688385\n",
      "the loss is 0.13217265903949738\n",
      "the loss is 0.21214226633310318\n",
      "the loss is 0.2597239390015602\n",
      "the loss is 0.35131238400936127\n",
      "the loss is 0.4036027640104294\n",
      "the loss is 0.4036027640104294\n",
      "the loss is 0.4061274901032448\n",
      "the loss is 0.04781292378902435\n",
      "the loss is 0.07942332327365875\n",
      "the loss is 0.08242252073250711\n",
      "Epoch2 finished\n",
      "acc: 0.01\n",
      "correct: 5\n",
      "lr: 0.001\n",
      "\n",
      "循环 4/20\n",
      "----------\n",
      "the loss is 0.03211865574121475\n",
      "the loss is 0.1292019784450531\n",
      "the loss is 0.17368587851524353\n",
      "the loss is 0.18424898386001587\n",
      "the loss is 0.2827077880501747\n",
      "the loss is 0.2827077880501747\n",
      "the loss is 0.28764108568429947\n",
      "the loss is 0.2933749482035637\n",
      "the loss is 0.3420756831765175\n",
      "the loss is 0.3541872724890709\n",
      "the loss is 0.025230981409549713\n",
      "the loss is 0.025230981409549713\n",
      "the loss is 0.20891491323709488\n",
      "the loss is 0.23675667494535446\n",
      "the loss is 0.29733993858098984\n",
      "the loss is 0.29733993858098984\n",
      "the loss is 0.35082119703292847\n",
      "the loss is 0.3994210511445999\n",
      "the loss is 0.42220866680145264\n",
      "the loss is 0.4312945753335953\n",
      "the loss is 0.14239057898521423\n",
      "the loss is 0.2532748132944107\n",
      "the loss is 0.32243845611810684\n",
      "the loss is 0.3713470920920372\n",
      "the loss is 0.4125770479440689\n",
      "the loss is 0.46601852774620056\n",
      "the loss is 0.5167429968714714\n",
      "the loss is 0.5348529294133186\n",
      "the loss is 0.5461776852607727\n",
      "the loss is 0.6106851324439049\n",
      "the loss is 0.008999377489089966\n",
      "the loss is 0.04615423083305359\n",
      "the loss is 0.10231166332960129\n",
      "the loss is 0.10514327883720398\n",
      "the loss is 0.1533704549074173\n",
      "the loss is 0.19622809439897537\n",
      "the loss is 0.25479645282030106\n",
      "the loss is 0.2674633637070656\n",
      "the loss is 0.29068996012210846\n",
      "the loss is 0.3438410013914108\n",
      "the loss is 0.056308358907699585\n",
      "the loss is 0.10649111866950989\n",
      "the loss is 0.172451414167881\n",
      "the loss is 0.1817353293299675\n",
      "the loss is 0.30544281750917435\n",
      "the loss is 0.3671976253390312\n",
      "the loss is 0.3819773718714714\n",
      "the loss is 0.3819773718714714\n",
      "the loss is 0.39960290491580963\n",
      "the loss is 0.4070340320467949\n",
      "the loss is 0.014672055840492249\n",
      "the loss is 0.022640451788902283\n",
      "the loss is 0.05944656580686569\n",
      "the loss is 0.07970590889453888\n",
      "the loss is 0.07970590889453888\n",
      "the loss is 0.10475163161754608\n",
      "the loss is 0.15418899059295654\n",
      "the loss is 0.18809718638658524\n",
      "the loss is 0.19558938592672348\n",
      "the loss is 0.19558938592672348\n",
      "the loss is 0.025003746151924133\n",
      "the loss is 0.04376142472028732\n",
      "the loss is 0.04376142472028732\n",
      "Epoch3 finished\n",
      "acc: 0.002\n",
      "correct: 1\n",
      "lr: 0.001\n",
      "\n",
      "循环 5/20\n",
      "----------\n",
      "the loss is 0.02113775908946991\n",
      "the loss is 0.02113775908946991\n",
      "the loss is 0.08803487569093704\n",
      "the loss is 0.18908964097499847\n",
      "the loss is 0.18908964097499847\n",
      "the loss is 0.5307718962430954\n",
      "the loss is 0.6506419405341148\n",
      "the loss is 0.6871315613389015\n",
      "the loss is 0.7721515595912933\n",
      "the loss is 0.8433673083782196\n",
      "the loss is 0.0\n",
      "the loss is 0.07259541749954224\n",
      "the loss is 0.24528823792934418\n",
      "the loss is 0.28949619084596634\n",
      "the loss is 0.3055097609758377\n",
      "the loss is 0.36918534338474274\n",
      "the loss is 0.3798569440841675\n",
      "the loss is 0.4328058063983917\n",
      "the loss is 0.49630334973335266\n",
      "the loss is 0.576182410120964\n",
      "the loss is 0.10535141080617905\n",
      "the loss is 0.10535141080617905\n",
      "the loss is 0.15099667012691498\n",
      "the loss is 0.17444869130849838\n",
      "the loss is 0.19758444279432297\n",
      "the loss is 0.24538765102624893\n",
      "the loss is 0.3212433010339737\n",
      "the loss is 0.32984963059425354\n",
      "the loss is 0.33744868636131287\n",
      "the loss is 0.3618820905685425\n",
      "the loss is 0.004158690571784973\n",
      "the loss is 0.026481300592422485\n",
      "the loss is 0.029506132006645203\n",
      "the loss is 0.10006680339574814\n",
      "the loss is 0.15901032835245132\n",
      "the loss is 0.18394438922405243\n",
      "the loss is 0.19271326810121536\n",
      "the loss is 0.19271326810121536\n",
      "the loss is 0.19271326810121536\n",
      "the loss is 0.2432631328701973\n",
      "the loss is 0.043037042021751404\n",
      "the loss is 0.11262267827987671\n",
      "the loss is 0.11262267827987671\n",
      "the loss is 0.1424538940191269\n",
      "the loss is 0.14456802606582642\n",
      "the loss is 0.15633368492126465\n",
      "the loss is 0.16685740649700165\n",
      "the loss is 0.21812252700328827\n",
      "the loss is 0.2541024386882782\n",
      "the loss is 0.35180486738681793\n",
      "the loss is 0.11261538416147232\n",
      "the loss is 0.30421457439661026\n",
      "the loss is 0.36230797320604324\n",
      "the loss is 0.49713022261857986\n",
      "the loss is 0.5540163889527321\n",
      "the loss is 0.5756387338042259\n",
      "the loss is 0.5778502151370049\n",
      "the loss is 0.6264333799481392\n",
      "the loss is 0.6264333799481392\n",
      "the loss is 0.6684405878186226\n",
      "the loss is 0.0\n",
      "the loss is 0.0047227442264556885\n",
      "the loss is 0.0047227442264556885\n",
      "Epoch4 finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f1a5bd835b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lhy/anaconda3/envs/squeezenet/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.016\n",
      "correct: 8\n",
      "lr: 0.001\n",
      "\n",
      "循环 6/20\n",
      "----------\n",
      "the loss is 0.023313023149967194\n",
      "the loss is 0.03483925759792328\n",
      "the loss is 0.0405062735080719\n",
      "the loss is 0.05535769462585449\n",
      "the loss is 0.1580553874373436\n",
      "the loss is 0.17215154320001602\n",
      "the loss is 0.17215154320001602\n",
      "the loss is 0.28834231197834015\n",
      "the loss is 0.28834231197834015\n",
      "the loss is 0.3542257845401764\n",
      "the loss is 0.09027640521526337\n",
      "the loss is 0.09027640521526337\n",
      "the loss is 0.09027640521526337\n",
      "the loss is 0.11901484429836273\n",
      "the loss is 0.2272922396659851\n",
      "the loss is 0.27175746858119965\n",
      "the loss is 0.27175746858119965\n",
      "the loss is 0.2911672443151474\n",
      "the loss is 0.38287481665611267\n",
      "the loss is 0.4086630195379257\n",
      "the loss is 0.0\n",
      "the loss is 0.004060432314872742\n",
      "the loss is 0.009870216250419617\n",
      "the loss is 0.14102958142757416\n",
      "the loss is 0.14102958142757416\n",
      "the loss is 0.17163527011871338\n",
      "the loss is 0.2632328122854233\n",
      "the loss is 0.29239600896835327\n",
      "the loss is 0.375115804374218\n",
      "the loss is 0.4600604623556137\n",
      "the loss is 0.05978837609291077\n",
      "the loss is 0.06323298811912537\n",
      "the loss is 0.06462974846363068\n",
      "the loss is 0.08014422655105591\n",
      "the loss is 0.12412650883197784\n",
      "the loss is 0.14896494150161743\n",
      "the loss is 0.23197918385267258\n",
      "the loss is 0.2715195491909981\n",
      "the loss is 0.31833312660455704\n",
      "the loss is 0.3214854374527931\n",
      "the loss is 0.002125173807144165\n",
      "the loss is 0.01326259970664978\n",
      "the loss is 0.038125619292259216\n",
      "the loss is 0.11300385743379593\n",
      "the loss is 0.15897173434495926\n",
      "the loss is 0.26854655891656876\n",
      "the loss is 0.2824264094233513\n",
      "the loss is 0.31272798031568527\n",
      "the loss is 0.3476533219218254\n",
      "the loss is 0.40239767730236053\n",
      "the loss is 0.0\n",
      "the loss is 0.0\n",
      "the loss is 0.0\n",
      "the loss is 0.03893638402223587\n",
      "the loss is 0.11570041626691818\n",
      "the loss is 0.14831062406301498\n",
      "the loss is 0.16024235635995865\n",
      "the loss is 0.16024235635995865\n",
      "the loss is 0.16857042163610458\n",
      "the loss is 0.17129776626825333\n",
      "the loss is 0.023805350065231323\n",
      "the loss is 0.023805350065231323\n",
      "the loss is 0.023805350065231323\n",
      "Epoch5 finished\n",
      "acc: 0.012\n",
      "correct: 6\n",
      "lr: 0.001\n",
      "\n",
      "循环 7/20\n",
      "----------\n",
      "the loss is 0.0\n",
      "the loss is 0.0\n",
      "the loss is 0.0\n",
      "the loss is 0.013820618391036987\n",
      "the loss is 0.0800737589597702\n",
      "the loss is 0.09985505044460297\n",
      "the loss is 0.16535193473100662\n",
      "the loss is 0.17878281325101852\n",
      "the loss is 0.2024032399058342\n",
      "the loss is 0.23340632766485214\n",
      "the loss is 0.06899823993444443\n",
      "the loss is 0.06935682147741318\n",
      "the loss is 0.06935682147741318\n",
      "the loss is 0.11118503659963608\n",
      "the loss is 0.11118503659963608\n",
      "the loss is 0.1153453066945076\n",
      "the loss is 0.129900760948658\n",
      "the loss is 0.17547135055065155\n",
      "the loss is 0.17547135055065155\n",
      "the loss is 0.20558218657970428\n",
      "the loss is 0.11312747001647949\n",
      "the loss is 0.18442434072494507\n",
      "the loss is 0.1895507276058197\n",
      "the loss is 0.20015878230333328\n",
      "the loss is 0.2232958748936653\n",
      "the loss is 0.2232958748936653\n",
      "the loss is 0.258437879383564\n",
      "the loss is 0.276080921292305\n",
      "the loss is 0.276080921292305\n",
      "the loss is 0.3703105300664902\n",
      "the loss is 0.029170207679271698\n",
      "the loss is 0.0346665233373642\n",
      "the loss is 0.12197589129209518\n",
      "the loss is 0.1327972635626793\n",
      "the loss is 0.14691482484340668\n",
      "the loss is 0.14962497353553772\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     13\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     15\u001b[0m     anchor, positive, negative \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m     anchor, positive, negative \u001b[38;5;241m=\u001b[39m anchor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), positive\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), negative\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/squeezenet/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/squeezenet/lib/python3.10/site-packages/torch/autograd/profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 622\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/anaconda3/envs/squeezenet/lib/python3.10/site-packages/torch/_ops.py:513\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter('runs/triplet_loss_experiment')\n",
    "# writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('Start Training')\n",
    "print('-'*10)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\n循环 {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        anchor, positive, negative = data[0]\n",
    "        anchor, positive, negative = anchor.to('cuda'), positive.to('cuda'), negative.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        anchor_out = model(anchor)\n",
    "        positive_out = model(positive)\n",
    "        negative_out = model(negative)\n",
    "        loss_val = loss(anchor_out, positive_out, negative_out)\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss_val.item()\n",
    "        # writer.iteration += 1\n",
    "        print(f\"the loss is {running_loss}\")\n",
    "        if i % 10 == 9:\n",
    "            # writer.add_scalar('loss', running_loss / 10,\n",
    "            #                   writer.iteration)\n",
    "            running_loss = 0.0\n",
    "    print('Epoch{} finished'.format(epoch))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    ## we use acc to evaluate the model\n",
    "    with torch.no_grad():\n",
    "        # 用标签来评估模型\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        reference_embeddings_np = np.vstack([embedding.cpu().numpy() for embedding in reference_embeddings])\n",
    "        near_nn = NearestNeighbors(n_neighbors=1, algorithm='auto',metric='euclidean',).fit(reference_embeddings_np)\n",
    "        # use labels to evaluate the model\n",
    "        for i,data in enumerate(test_loader,0):\n",
    "                anchor, _, _ = data[0]\n",
    "                anchor_label, _, _ = data[1]\n",
    "                batch_sizes = anchor.size(0)\n",
    "                total += batch_sizes\n",
    "                anchor = anchor.to('cuda')\n",
    "                # print(anchor.shape)\n",
    "                anchor_out = model(anchor).cpu().detach().numpy()\n",
    "                \n",
    "                distances, indices = near_nn.kneighbors(anchor_out)\n",
    "                indices = indices.reshape(-1)\n",
    "                \n",
    "                for j in range(batch_sizes):\n",
    "                    if reference_labels[indices[j]] == anchor_label[j]:\n",
    "                        correct += 1\n",
    "        acc = correct / total\n",
    "        print('acc:', acc)\n",
    "        print('correct:', correct)\n",
    "        \n",
    "        # writer.add_scalar('acc', acc, epoch)\n",
    "        \n",
    "        \n",
    "        # 用scheduler来更新学习率\n",
    "        scheduler.step(acc)\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        print(\"Current learning rate: \", current_lr)\n",
    "        print('lr:', optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "print('Finished Training')\n",
    "\n",
    "# writer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    \n",
    "        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "              \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "squeezenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
